import os
import time
import json
import config 
import pandas as pd
import urllib.request
import multiprocessing
from datetime import datetime, timedelta, date
from binance_historical_data import BinanceDataDumper
from sqlalchemy.orm import sessionmaker
from sqlalchemy import Table, Column, MetaData, DateTime, Float, Integer, String, PrimaryKeyConstraint, Date, inspect, insert, update

# Khai b√°o metadata
metadata = MetaData()

# ƒê·ªãnh nghƒ©a b·∫£ng `tickers`
tickers_table = Table(
    'tickers', metadata,
    Column('ticker', String(50), primary_key=True),
    Column('first_open_time', Date, nullable=False),
    Column('last_updated_date', Date, nullable=False),
    Column('name', String(50), nullable=False)
)

# H√†m ƒë·ªÉ l·∫•y danh s√°ch c√°c c·∫∑p giao d·ªãch t·ª´ Binance
def get_list_all_trading_pairs():
    # S·ª≠ d·ª•ng BinanceDataDumper ƒë·ªÉ l·∫•y danh s√°ch c√°c c·∫∑p giao d·ªãch
    data_dumper = BinanceDataDumper(
        path_dir_where_to_dump=".",
        asset_class="spot",  # L·∫•y c√°c c·∫∑p giao d·ªãch spot
        data_type="klines",  # Lo·∫°i d·ªØ li·ªáu (kh√¥ng quan tr·ªçng ·ªü ƒë√¢y)
        data_frequency="1h",  # T·∫ßn su·∫•t d·ªØ li·ªáu (kh√¥ng quan tr·ªçng ·ªü ƒë√¢y)
    )
    # L·∫•y danh s√°ch c√°c c·∫∑p giao d·ªãch
    trading_pairs = data_dumper.get_list_all_trading_pairs()
    return trading_pairs

# H√†m ƒë·ªÉ l·ªçc c√°c tickers c√≥ ƒëu√¥i USDT
def filter_usdt_tickers(tickers):
    exclude_keywords = ["UPUSDT", "DOWNUSDT", "BEARUSDT", "BULLUSDT"]
    return [ticker for ticker in tickers if ticker.endswith("USDT") and not any(ex in ticker for ex in exclude_keywords)]

# H√†m ƒë·ªÉ t·∫°o t√™n b·∫£ng t·ª´ ticker
def get_table_name(ticker):
    return ticker.lower().replace("usdt", "_usdt")

# H√†m ƒë·ªÉ ki·ªÉm tra v√† t·∫°o b·∫£ng `tickers` n·∫øu ch∆∞a t·ªìn t·∫°i
def ensure_tickers_table_exists():
    inspector = inspect(engine)  # S·ª≠ d·ª•ng inspect ƒë·ªÉ ki·ªÉm tra b·∫£ng
    if not inspector.has_table('tickers'):
        metadata.create_all(engine, [tickers_table])
        print("B·∫£ng 'tickers' ƒë√£ ƒë∆∞·ª£c t·∫°o.")

def get_tickers_data():
    """
    L·∫•y to√†n b·ªô d·ªØ li·ªáu `last_updated_date` v√† `first_open_time` t·ª´ b·∫£ng tickers d∆∞·ªõi d·∫°ng dictionary.
    N·∫øu b·∫£ng kh√¥ng t·ªìn t·∫°i, s·∫Ω t·ª± ƒë·ªông kh·ªüi t·∫°o b·∫£ng.
    Tr·∫£ v·ªÅ:
    - dict d·∫°ng {ticker: {'last_updated_date': date, 'first_open_time': date}}
    """
    try:
        ensure_tickers_table_exists()

        result = session.query(
            tickers_table.c.ticker,
            tickers_table.c.last_updated_date,
            tickers_table.c.first_open_time,
            tickers_table.c.name
        ).all()

        tickers_data = {
            row.ticker: {
                'last_updated_date': row.last_updated_date,
                'first_open_time': row.first_open_time,
                'name': row.name
            }
            for row in result
        }

        return tickers_data

    except Exception as e:
        print(f"L·ªói khi l·∫•y d·ªØ li·ªáu tickers: {str(e)}")
        return {}

# H√†m ƒë·ªÉ t√¨m ng√†y ƒë·∫ßu ti√™n c√≥ d·ªØ li·ªáu
def find_first_data_date(ticker):
    # S·ª≠ d·ª•ng BinanceDataDumper ƒë·ªÉ l·∫•y ng√†y ƒë·∫ßu ti√™n c√≥ d·ªØ li·ªáu
    data_dumper = BinanceDataDumper(
        path_dir_where_to_dump=".",
        asset_class="spot",  # L·∫•y d·ªØ li·ªáu spot
        data_type="klines",  # Lo·∫°i d·ªØ li·ªáu
        data_frequency="1h",  # T·∫ßn su·∫•t d·ªØ li·ªáu
    )
    # L·∫•y ng√†y ƒë·∫ßu ti√™n c√≥ d·ªØ li·ªáu
    min_start_date = data_dumper.get_min_start_date_for_ticker(ticker)
    return min_start_date

# H√†m ƒë·ªÉ t·∫°o b·∫£ng n·∫øu ch∆∞a t·ªìn t·∫°i
def create_table_if_not_exists(table_name):
    inspector = inspect(engine)
    if not inspector.has_table(table_name):
        table = Table(
            table_name, metadata,
            Column('open_time', DateTime, primary_key=True),
            Column('open', Float, nullable=False),
            Column('high', Float, nullable=False),
            Column('low', Float, nullable=False),
            Column('close', Float, nullable=False),
            Column('volume', Float),
            Column('close_time', DateTime),
            Column('quote_asset_volume', Float),
            Column('number_of_trades', Integer),
            Column('taker_buy_base_asset_volume', Float),
            Column('taker_buy_quote_asset_volume', Float),
            Column('ignore', Float)
        )
        metadata.create_all(engine)
        print(f"B·∫£ng '{table_name}' ƒë√£ ƒë∆∞·ª£c t·∫°o.")

# H√†m ƒë·ªÉ l∆∞u d·ªØ li·ªáu v√†o b·∫£ng
def save_data_to_table(table_name, data):
    data_dict = data.to_dict(orient='records')
    if not data_dict:
        return
    
    table = Table(table_name, metadata, autoload_with=engine)
    statement = insert(table).prefix_with("IGNORE").values(data_dict)
    session.execute(statement)

# H√†m ƒë·ªÉ c·∫≠p nh·∫≠t last_updated_date trong b·∫£ng tickers
def update_tickers_table(ticker, first_open_time, last_updated_date, name):
    session.execute(insert(tickers_table)
                    .prefix_with("IGNORE")
                    .values(
                        ticker=ticker,
                        first_open_time=first_open_time,
                        last_updated_date=last_updated_date,
                        name=name
                        ))

# H√†m ƒë·ªÉ l·∫•y t·∫•t c·∫£ c√°c file CSV t·ª´ m·ªôt th∆∞ m·ª•c
def get_csv_files(directory):
    try:
        if not os.path.exists(directory):
            print(f"Warning: Th∆∞ m·ª•c kh√¥ng t·ªìn t·∫°i: {directory}")
            return []  # Tr·∫£ v·ªÅ danh s√°ch r·ªóng thay v√¨ raise error
        return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.csv')]
    except Exception as e:
        print(f"L·ªói khi ƒë·ªçc th∆∞ m·ª•c {directory}: {str(e)}")
        return []

# H√†m ƒë·ªÉ x√°c ƒë·ªãnh ƒë∆°n v·ªã c·ªßa timestamp
def detect_timestamp_unit(timestamp):
    # ƒê·∫øm s·ªë ch·ªØ s·ªë c·ªßa timestamp
    num_digits = len(str(timestamp))
    
    if num_digits == 13:
        return 'ms'  # Milli gi√¢y
    elif num_digits == 16:
        return 'us'  # Micro gi√¢y
    else:
        raise ValueError(f"Timestamp kh√¥ng h·ª£p l·ªá: {timestamp}")
    
# H√†m ƒë·ªÉ chuy·ªÉn ƒë·ªïi timestamp sang datetime
def convert_timestamp(timestamp):
    unit = detect_timestamp_unit(timestamp)
    return pd.to_datetime(timestamp, unit=unit, errors='coerce')

def read_csv_file(file_path):
    # ƒê·ªçc file CSV
    df = pd.read_csv(file_path)
    
    df.columns = [
        "open_time",  
        "open",     
        "high",       
        "low",     
        "close",    
        "volume", 
        "close_time", 
        "quote_asset_volume",      
        "number_of_trades",        
        "taker_buy_base_asset_volume",  
        "taker_buy_quote_asset_volume", 
        "ignore"                     
        ]
    # Chuy·ªÉn ƒë·ªïi open_time v√† close_time sang ki·ªÉu datetime
    df['open_time'] = df['open_time'].apply(convert_timestamp)
    df['close_time'] = df['close_time'].apply(convert_timestamp)
    
    return df

def download_ticker(ticker, date_start):
    """H√†m th·ª±c hi·ªán t·∫£i d·ªØ li·ªáu cho m·ªôt ticker c·ª• th·ªÉ"""
    data_dumper = BinanceDataDumper(
        path_dir_where_to_dump=".",
        asset_class="spot",
        data_type="klines",
        data_frequency="1h",
    )
    data_dumper.dump_data(
        tickers=ticker,
        date_start=date_start,
        date_end=date.today(),
        is_to_update_existing=False,
    )

def format_time(seconds):
    """Chuy·ªÉn ƒë·ªïi gi√¢y th√†nh ƒë·ªãnh d·∫°ng hh:mm:ss."""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    return f"{hours:02d}:{minutes:02d}:{secs:02d}"

# H√†m ch√≠nh
def main():
    start_time = time.time()
    print("B·∫Øt ƒë·∫ßu x·ª≠ l√Ω tickers...")

    tickers = filter_usdt_tickers(get_list_all_trading_pairs())
    print(f"L·∫•y danh s√°ch tickers: {len(tickers)} tickers, th·ªùi gian: {format_time(time.time() - start_time)}")

    tickers_data = get_tickers_data()
    print(f"L·∫•y d·ªØ li·ªáu tickers: th·ªùi gian: {format_time(time.time() - start_time)}")

    tickers_update_info = [] # Kh·ªüi t·∫°o list ƒë·ªÉ sort sau khi update

    total_tickers = len(tickers)
    for index, ticker in enumerate(tickers):
        ticker_start_time = time.time()

        table_name = get_table_name(ticker)
        data_info = tickers_data.get(ticker, {})
        first_open_time = data_info.get('first_open_time')

        if first_open_time is None:
            first_open_time = find_first_data_date(ticker)
            if first_open_time is None:
                print(f"B·ªè qua {ticker} do kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ƒë·∫ßu ti√™n.")
                continue

        last_updated_date = data_info.get('last_updated_date', first_open_time)

        if last_updated_date != first_open_time:
            last_updated_date = last_updated_date - timedelta(days=1)

        try:
            update_tickers_table(ticker, first_open_time, last_updated_date, table_name)
            session.commit()
            tickers_update_info.append({ # Th√™m v√†o list ƒë·ªÉ sort
                'ticker': ticker,
                'last_updated_date': last_updated_date,
                'name': table_name
            })
            print(f"{ticker}: {format_time(time.time() - ticker_start_time)}, {index + 1}/{total_tickers}", end="")
            avg_ticker_time = (time.time() - start_time) / (index + 1)
            remaining_tickers = total_tickers - index - 1
            estimated_time = avg_ticker_time * remaining_tickers
            print(f", ∆Ø·ªõc t√≠nh c√≤n l·∫°i: {format_time(estimated_time)}")
        except Exception as e:
            session.rollback()
            print(f"L·ªói khi c·∫≠p nh·∫≠t {ticker}: {str(e)}, th·ªùi gian: {format_time(time.time() - ticker_start_time)}")
            continue

    # S·∫Øp x·∫øp ticker theo ng√†y c·∫≠p nh·∫≠t g·∫ßn nh·∫•t
    tickers_update_info.sort(key=lambda x: x['last_updated_date'])
    tickers = [info['ticker'] for info in tickers_update_info]

    print(f"S·∫Øp x·∫øp tickers theo ng√†y c·∫≠p nh·∫≠t: th·ªùi gian: {format_time(time.time() - start_time)}")
    print(f"Ho√†n th√†nh x·ª≠ l√Ω v√† c·∫≠p nh·∫≠t tickers, t·ªïng th·ªùi gian: {format_time(time.time() - start_time)}")
    
    # Duy·ªát qua t·ª´ng ticker
    for i, ticker in enumerate(tickers):
        start_time = time.time()
        print(f"üîÑ ƒêang x·ª≠ l√Ω {ticker}... (Ticker {i + 1}/{len(tickers)})")
        ticker_info = next((info for info in tickers_update_info if info['ticker'] == ticker), {})

        date_start = ticker_info.get('last_updated_date')
        if not date_start:
            date_start = find_first_data_date(ticker)
        else:
            date_start = date_start + timedelta(days=-1)

        try:
            result = download_ticker(ticker, date_start)
            elapsed_time = time.time() - start_time
            completed_times.append(elapsed_time)

            if elapsed_time > 60:
                print(f"‚ö†Ô∏è B·ªè qua {ticker}: Th·ªùi gian ch·∫°y v∆∞·ª£t qu√° 60 gi√¢y ({format_time(elapsed_time)})")
                continue
            print(f"‚úÖ Th√†nh c√¥ng {ticker}: {result} (Th·ªùi gian: {format_time(elapsed_time)})")
        except Exception as e:
            print(f"‚ùå L·ªói {ticker}: {e}")

        daily_files = os.path.join(os.getcwd(), f"spot/daily/klines/{ticker}/1h")
        monthly_files = os.path.join(os.getcwd(), f"spot/monthly/klines/{ticker}/1h")

        daily_files = get_csv_files(daily_files)
        monthly_files = get_csv_files(monthly_files)
        all_files = daily_files + monthly_files

        if not all_files:
            print(f"‚ùó Kh√¥ng c√≥ file CSV n√†o cho {ticker}")
            continue

        data = pd.concat([read_csv_file(file) for file in all_files], ignore_index=True)
        data.sort_values(by='open_time', inplace=True)

        table_name = ticker_info.get('name')
        create_table_if_not_exists(table_name)

        try:
            save_data_to_table(table_name, data)
            update_tickers_table(ticker, first_open_time, date.today(), table_name)
            session.commit()
        except FileNotFoundError as e:
            print(f"‚ùó L·ªói kh√¥ng t√¨m th·∫•y file cho {ticker}: {str(e)}")
            continue
        except Exception as e:
            print(f"‚ùó L·ªói nghi√™m tr·ªçng khi x·ª≠ l√Ω {ticker}: {str(e)}")
            session.rollback()
            continue
        finally:
            session.close()

        remaining_tickers = len(tickers) - (i + 1)
        print(f"‚è≥ ∆Ø·ªõc t√≠nh th·ªùi gian c√≤n l·∫°i: {estimate_remaining_time(completed_times, remaining_tickers)}")

# Ch·∫°y h√†m ch√≠nh
if __name__ == "__main__":
    # Kh·ªüi t·∫°o engine v√† session
    engine = config.create_database_engine()
    Session = sessionmaker(bind=engine)
    session = Session()

    main()
    
    session.close()